# -*- coding: utf-8 -*-
"""Copy of NLPProject_Word2Vec_NewB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YbMDxo7F-8L7_W-jw1pxnM7FSSodn7mF
"""

from google.colab import drive
drive.mount('/content/drive')

from os import path as osp
from tqdm import tqdm

import numpy as np
import pandas as pd

import gensim
from gensim.test.utils import common_texts
from gensim.models.word2vec import Word2Vec

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.utils.data
import torchtext.data
import warnings
from sklearn.metrics import accuracy_score
from torch.autograd import Variable

root_folder = "/content/drive/MyDrive/NLP/NewB-master/"
pretrained_embeddings_path = "/content/drive/MyDrive/NLP/GoogleNews-vectors-negative300.bin"

embedding_model = gensim.models.KeyedVectors.load_word2vec_format(pretrained_embeddings_path, binary=True)

a = embedding_model.get_vector('king') - embedding_model.get_vector('man') + embedding_model.get_vector('woman')
b = embedding_model.get_vector('queen')

from numpy import dot
from numpy.linalg import norm
cos_sim = dot(a, b) / (norm(a)*norm(b))
print(cos_sim)

def read_corpus(fname):
    f = pd.read_table(osp.join(root_folder, fname), header=None)
    for i, line in enumerate(f[1]):
        tokens = gensim.utils.simple_preprocess(line)
        yield tokens

def read_labels(fname):
    f = pd.read_table(osp.join(root_folder, fname), header=None)
    for i, line in enumerate(f[0]):
        yield int(line)

train_corpus = list(read_corpus("train_orig.txt"))
y_train_corpus = np.asarray(list(read_labels("train_orig.txt"))).astype(np.float32)
test_corpus = list(read_corpus("test.txt"))
y_test_corpus = np.asarray(list(read_labels("test.txt"))).astype(np.float32)

all_corpus = train_corpus + test_corpus

# model = Word2Vec(vector_size=200, min_count=10)
# model.build_vocab([x for x in tqdm(all_corpus)])
# model.train([x for x in tqdm(all_corpus)], total_examples=len(all_corpus), epochs=10)

from keras.utils import to_categorical

# Convert ground truth labels to categorical form
y_train_categorical = to_categorical(y_train_corpus)
y_test_categorical = to_categorical(y_test_corpus)

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)
matrix = vectorizer.fit_transform([x for x in all_corpus])
tfidf = dict(zip(vectorizer.get_feature_names_out(), vectorizer.idf_))

def build_Word_Vector(tokens, size):
    vec = np.zeros(size).reshape((1, size))
    count = 0.
    for word in tokens:
        try:
            vec += embedding_model.get_vector(word).reshape((1, size)) * tfidf[word]
            count += 1.
        except KeyError: 
            
            continue
    if count != 0:
        vec /= count
    return vec

from sklearn.preprocessing import scale
import numpy as np

train_vecs_w2v = np.concatenate([build_Word_Vector(z, 300) for z in tqdm(map(lambda x: x, train_corpus))])
val_vecs_w2v = np.concatenate([build_Word_Vector(z, 300) for z in tqdm(map(lambda x: x, test_corpus))])

from keras.models import Sequential
from keras.layers import Dense

model = Sequential()
model.add(Dense(256, activation='relu', input_dim=300))
model.add(Dense(256, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dense(11, activation='softmax'))
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])

model.fit(train_vecs_w2v, y_train_categorical, epochs=30, batch_size=32, verbose=2)

for n in range(1,6):
  preds = model.predict(val_vecs_w2v, batch_size=128, verbose=2)
  top_n = np.mean([np.any(np.argsort(pred)[-n:] == np.argmax(true)) for pred, true in zip(preds, y_test_categorical)])
  print(f"Top-{n} Accuracy: {top_n}")